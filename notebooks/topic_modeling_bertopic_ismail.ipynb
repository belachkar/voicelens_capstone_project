{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b52e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet bertopic sentence-transformers umap-learn hdbscan gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5246a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# ðŸ“¦ Setup & Dependencies\n",
    "# ================================\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Utils\n",
    "from typing import List\n",
    "\n",
    "# Load spaCy model (English)\n",
    "# For production, prefer: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cae58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text: lowercase, remove URLs, punctuation, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize text using spaCy, removing stopwords & short tokens.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.lemma_ not in STOPWORDS\n",
    "        and len(token.lemma_) > 2\n",
    "        and token.lemma_.isalpha()\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4483a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame, text_column: str = \"comment\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply full preprocessing pipeline on the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "        text_column (str): column containing raw text\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with processed_text column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Clean\n",
    "    df[\"clean_text\"] = df[text_column].apply(clean_text)\n",
    "\n",
    "    # Lemmatize\n",
    "    df[\"processed_text\"] = df[\"clean_text\"].apply(lemmatize_text)\n",
    "\n",
    "    # Drop rows where processed text is empty\n",
    "    df = df[df[\"processed_text\"].str.strip() != \"\"]\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "078167a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>uncaring and incompetent impossible to deal wi...</td>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon maybe the quickest way to get amazon ma...</td>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>not fair in genera i am an amazon junkie i lov...</td>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon prime is crap amazon prime is crap firs...</td>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>terrible delivery services terrible delivery s...</td>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  uncaring and incompetent impossible to deal wi...   \n",
       "1  amazon maybe the quickest way to get amazon ma...   \n",
       "2  not fair in genera i am an amazon junkie i lov...   \n",
       "3  amazon prime is crap amazon prime is crap firs...   \n",
       "4  terrible delivery services terrible delivery s...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  uncaring incompetent impossible deal customer ...  \n",
       "1  amazon maybe quick way get amazon maybe quick ...  \n",
       "2  fair genera amazon junkie love tthose package ...  \n",
       "3  amazon prime crap amazon prime crap first orde...  \n",
       "4  terrible delivery service terrible delivery se...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = preprocess_dataframe(df, text_column=\"comment\")\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be472105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---- Embedding Model for BERTopic ----\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_embedding_model(model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load a sentence embedding model for BERTopic.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the SentenceTransformer model.\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name} ...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"Embedding model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "embedding_model = load_embedding_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic model successfully initialized.\n"
     ]
    }
   ],
   "source": [
    "# ---- Topic Modeling Step ----\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "def build_topic_model(\n",
    "    embedding_model,\n",
    "    n_neighbors: int = 15,\n",
    "    n_components: int = 5,\n",
    "    min_cluster_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a BERTopic model with UMAP + HDBSCAN + SentenceTransformer embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: Preloaded SentenceTransformer model.\n",
    "        n_neighbors (int): UMAP neighbors.\n",
    "        n_components (int): UMAP dimensions.\n",
    "        min_cluster_size (int): Minimum cluster size for HDBSCAN.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: Configured BERTopic model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dimensionality Reduction\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Clustering\n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    # BERTopic Model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"BERTopic model successfully initialized.\")\n",
    "    return topic_model\n",
    "\n",
    "\n",
    "# ---- Build the model ----\n",
    "topic_model = build_topic_model(embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33460e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 17:48:41,743 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic on 12948 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 405/405 [02:01<00:00,  3.33it/s]\n",
      "2025-11-18 17:50:43,758 - BERTopic - Embedding - Completed âœ“\n",
      "2025-11-18 17:50:43,760 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-18 17:51:39,620 - BERTopic - Dimensionality - Completed âœ“\n",
      "2025-11-18 17:51:39,623 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-18 17:51:41,168 - BERTopic - Cluster - Completed âœ“\n",
      "2025-11-18 17:51:41,189 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-18 17:51:42,144 - BERTopic - Representation - Completed âœ“\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---- Fit BERTopic on your processed text ----\n",
    "\n",
    "def fit_topic_model(topic_model, df, text_column=\"processed_text\"):\n",
    "    \"\"\"\n",
    "    Fit BERTopic on cleaned text data.\n",
    "\n",
    "    Args:\n",
    "        topic_model: BERTopic model instance.\n",
    "        df (DataFrame): Preprocessed dataframe.\n",
    "        text_column (str): Column containing processed text.\n",
    "\n",
    "    Returns:\n",
    "        topics, probabilities, topic_info\n",
    "    \"\"\"\n",
    "    docs = df[text_column].tolist()\n",
    "    print(f\"Fitting BERTopic on {len(docs)} documents...\")\n",
    "\n",
    "    topics, probabilities = topic_model.fit_transform(docs)\n",
    "\n",
    "    print(\"Model fitted successfully.\")\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    return topics, probabilities, topic_info\n",
    "\n",
    "\n",
    "# ---- Run the fitting ----\n",
    "topics, probs, topic_info = fit_topic_model(topic_model, df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af975b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>uncaring and incompetent impossible to deal wi...</td>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon maybe the quickest way to get amazon ma...</td>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.927613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>not fair in genera i am an amazon junkie i lov...</td>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.885888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon prime is crap amazon prime is crap firs...</td>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>terrible delivery services terrible delivery s...</td>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  uncaring and incompetent impossible to deal wi...   \n",
       "1  amazon maybe the quickest way to get amazon ma...   \n",
       "2  not fair in genera i am an amazon junkie i lov...   \n",
       "3  amazon prime is crap amazon prime is crap firs...   \n",
       "4  terrible delivery services terrible delivery s...   \n",
       "\n",
       "                                      processed_text  dominant_topic  \\\n",
       "0  uncaring incompetent impossible deal customer ...               0   \n",
       "1  amazon maybe quick way get amazon maybe quick ...               0   \n",
       "2  fair genera amazon junkie love tthose package ...               0   \n",
       "3  amazon prime crap amazon prime crap first orde...               0   \n",
       "4  terrible delivery service terrible delivery se...               0   \n",
       "\n",
       "   topic_probability  \n",
       "0           0.825409  \n",
       "1           0.927613  \n",
       "2           0.885888  \n",
       "3           0.980481  \n",
       "4           0.968909  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_topics_to_df(df, topics, probabilities):\n",
    "    \"\"\"\n",
    "    Assigns BERTopic output back to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Preprocessed dataframe\n",
    "        topics (list[int]): Topic numbers for each document\n",
    "        probabilities (np.ndarray or list): Probabilities from BERTopic\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with added columns:\n",
    "            - dominant_topic\n",
    "            - topic_probability (probability of dominant topic)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"dominant_topic\"] = topics\n",
    "\n",
    "    # If probabilities is 2D, take probability of the assigned topic\n",
    "    if isinstance(probabilities, np.ndarray) and probabilities.ndim == 2:\n",
    "        probs_for_dominant = [probabilities[i, t] if t >= 0 else 0.0\n",
    "                              for i, t in enumerate(topics)]\n",
    "        df[\"topic_probability\"] = probs_for_dominant\n",
    "    else:\n",
    "        df[\"topic_probability\"] = probabilities\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_topics = assign_topics_to_df(df_clean, topics, probs)\n",
    "df_topics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_labels(topic_model, topic_info, top_n_words: int = 3):\n",
    "    \"\"\"\n",
    "    Generate human-readable topic labels using BERTopic's top words.\n",
    "\n",
    "    Args:\n",
    "        topic_model: Fitted BERTopic model.\n",
    "        topic_info (DataFrame): topic_model.get_topic_info()\n",
    "        top_n_words (int): Number of representative words to include in the label.\n",
    "\n",
    "    Returns:\n",
    "        dict: {topic_id: topic_label}\n",
    "    \"\"\"\n",
    "    topic_labels = {}\n",
    "\n",
    "    for topic_id in topic_info.Topic:\n",
    "        if topic_id == -1:\n",
    "            topic_labels[topic_id] = \"Outliers\"\n",
    "            continue\n",
    "\n",
    "        # Get top words for this topic\n",
    "        words = [word for word, _ in topic_model.get_topic(topic_id)[:top_n_words]]\n",
    "\n",
    "        # Make a short, readable label\n",
    "        label = \" \".join(words).title()\n",
    "        topic_labels[topic_id] = label\n",
    "\n",
    "    return topic_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a164cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>Topic</th>\n",
       "      <th>topic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.825409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.927613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.885888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.980481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.968909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day bait switch delivery nothing frustrating o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.897454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ever use amazone ever use amazone faulty item ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.750962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cost return two item cost return two item defe...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.948280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>order small kitchen appliance order small kitc...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.897604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amazon need quit call number amazon need quit ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.843111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text  dominant_topic  \\\n",
       "0  uncaring incompetent impossible deal customer ...               0   \n",
       "1  amazon maybe quick way get amazon maybe quick ...               0   \n",
       "2  fair genera amazon junkie love tthose package ...               0   \n",
       "3  amazon prime crap amazon prime crap first orde...               0   \n",
       "4  terrible delivery service terrible delivery se...               0   \n",
       "5  day bait switch delivery nothing frustrating o...               0   \n",
       "6  ever use amazone ever use amazone faulty item ...               0   \n",
       "7  cost return two item cost return two item defe...               0   \n",
       "8  order small kitchen appliance order small kitc...               0   \n",
       "9  amazon need quit call number amazon need quit ...               0   \n",
       "\n",
       "                     Topic  topic_probability  \n",
       "0  Amazon Service Customer           0.825409  \n",
       "1  Amazon Service Customer           0.927613  \n",
       "2  Amazon Service Customer           0.885888  \n",
       "3  Amazon Service Customer           0.980481  \n",
       "4  Amazon Service Customer           0.968909  \n",
       "5  Amazon Service Customer           0.897454  \n",
       "6  Amazon Service Customer           0.750962  \n",
       "7  Amazon Service Customer           0.948280  \n",
       "8  Amazon Service Customer           0.897604  \n",
       "9  Amazon Service Customer           0.843111  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "topic_labels = generate_topic_labels(topic_model, topic_info)\n",
    "\n",
    "# Add a new column 'Topic' with human-readable labels\n",
    "df_topics[\"Topic\"] = df_topics[\"dominant_topic\"].map(topic_labels)\n",
    "\n",
    "df_topics[[\"processed_text\", \"dominant_topic\", \"Topic\", \"topic_probability\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1dd7c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20  \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20  \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20  \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20  \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ismail/code/belachkar/voicelens_capstone_project/raw_data/review_for_amazon.csv\",\n",
    "                 encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5da9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d2778ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean a single text string.\n",
    "\n",
    "\n",
    "    Steps:\n",
    "    - Lowercase\n",
    "    - Remove URLs, emails\n",
    "    - Remove non-letter characters (keep spaces)\n",
    "    - Strip repeated spaces\n",
    "    - Basic normalisation\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text_for_tokens(text: str, stopwords_set=STOPWORDS) -> list:\n",
    "    \"\"\"Return a list of lemmatized tokens after cleaning.\n",
    "\n",
    "\n",
    "    This function is intentionally small and deterministic. It will be used for\n",
    "    both coherence calculations and for any token-based analysis.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if len(t) > 2 and t not in stopwords_set]\n",
    "    tokens = [LEMMATIZER.lemmatize(t) for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df: pd.DataFrame, text_col: str = 'comment') -> pd.DataFrame:\n",
    "    \"\"\"Preprocess dataframe in place and return a copy with a new column 'clean_text'.\n",
    "\n",
    "\n",
    "    - Drops rows with empty comments\n",
    "    - Adds 'clean_text' (string)\n",
    "    - Adds 'tokens' (list of tokens)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['clean_text'] = df[text_col].astype(str).apply(clean_text)\n",
    "    df = df[df['clean_text'].str.strip() != '']\n",
    "    df['tokens'] = df['clean_text'].apply(preprocess_text_for_tokens)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756d208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ismail/.pyenv/versions/voicelens/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2' # compact & good for BERTopic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_embeddings(text_list: list, model_name: str = EMBEDDING_MODEL_NAME, batch_size: int = 32):\n",
    "    \"\"\"Return numpy array of embeddings for a list of texts.\n",
    "\n",
    "\n",
    "    The function caches the model object (as an attribute) to avoid reloads when\n",
    "    called multiple times.\n",
    "    \"\"\"\n",
    "    if not hasattr(generate_embeddings, '_model'):\n",
    "        generate_embeddings._model = SentenceTransformer(model_name)\n",
    "    model = generate_embeddings._model\n",
    "    embeddings = model.encode(text_list, show_progress_bar=True, batch_size=batch_size)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0073c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "\n",
    "def train_bertopic_model(embeddings: np.ndarray,\n",
    "        texts: list,\n",
    "        n_neighbors: int = 15,\n",
    "        n_components: int = 5,\n",
    "        min_cluster_size: int = 15,\n",
    "        random_state: int = RANDOM_STATE):\n",
    "    \"\"\"Train a BERTopic model using UMAP + HDBSCAN and return the trained model.\n",
    "\n",
    "\n",
    "    Parameters are exposed to allow later tuning.\n",
    "    \"\"\"\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        metric='cosine',\n",
    "        random_state=random_state)\n",
    "\n",
    "\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom')\n",
    "\n",
    "\n",
    "    topic_model = BERTopic(umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=False,\n",
    "        verbose=True,\n",
    "        random_state=random_state)\n",
    "\n",
    "\n",
    "    topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e2dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topics(df: pd.DataFrame, model: BERTopic, embeddings: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Assign dominant_topic and human-readable Topic label to the dataframe.\n",
    "\n",
    "\n",
    "    Returns a copy of df with columns ['dominant_topic', 'Topic_label'] added.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    topics, probs = model.transform(df['clean_text'].tolist(), embeddings)\n",
    "    df['dominant_topic'] = topics\n",
    "\n",
    "\n",
    "    # Build human-readable labels using representative words\n",
    "    topic_info = model.get_topic_info()\n",
    "\n",
    "\n",
    "    def make_label(topic_id: int):\n",
    "        if topic_id == -1:\n",
    "            return 'Outlier'\n",
    "        top_words = [w for w, _ in model.get_topic(topic_id)][:5]\n",
    "        # heuristic: join first 2-3 words into a short label\n",
    "        label = ' '.join([w.capitalize() for w in top_words[:3]])\n",
    "        return label\n",
    "\n",
    "\n",
    "    df['Topic'] = df['dominant_topic'].apply(make_label)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6b09bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/ismail/.pyenv/versions/voicelens/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/ismail/.pyenv/versions/voicelens/nltk_data...\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ismail/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/share/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momw-1.4\u001b[39m\u001b[38;5;124m\"\u001b[39m, download_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ismail/.pyenv/versions/voicelens/nltk_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ismail/.pyenv/versions/voicelens/nltk_data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_clean\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[5], line 65\u001b[0m, in \u001b[0;36mpreprocess_dataframe\u001b[0;34m(df, text_col)\u001b[0m\n\u001b[1;32m     63\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[text_col]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m     64\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text_for_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mpreprocess_text_for_tokens\u001b[0;34m(text, stopwords_set)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of lemmatized tokens after cleaning.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mThis function is intentionally small and deterministic. It will be used for\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mboth coherence calculations and for any token-based analysis.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m text \u001b[38;5;241m=\u001b[39m clean_text(text)\n\u001b[0;32m---> 46\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords_set]\n\u001b[1;32m     48\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [LEMMATIZER\u001b[38;5;241m.\u001b[39mlemmatize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m~/.pyenv/versions/voicelens/lib/python3.10/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/home/ismail/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/share/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/ismail/.pyenv/versions/voicelens/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"wordnet\", download_dir=\"/home/ismail/.pyenv/versions/voicelens/nltk_data\")\n",
    "nltk.download(\"omw-1.4\", download_dir=\"/home/ismail/.pyenv/versions/voicelens/nltk_data\")\n",
    "nltk.data.path.append(\"/home/ismail/.pyenv/versions/voicelens/nltk_data\")\n",
    "\n",
    "df_clean = preprocess_dataframe(df)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "return topic_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_coherence_from_model(model: BERTopic, texts_tokens: list, top_n: int = 10):\n",
    "\"\"\"Compute coherence using Gensim's C_V coherence over topic top words.\n",
    "\n",
    "\n",
    "texts_tokens: list of token lists (preprocessed)\n",
    "\"\"\"\n",
    "topic_words = topic_top_words(model, top_n=top_n)\n",
    "# Build gensim dictionary and corpus\n",
    "dictionary = Dictionary(texts_tokens)\n",
    "# Gensim's CoherenceModel requires list of topic word lists\n",
    "topics_for_gensim = [topic_words[t] for t in sorted(topic_words.keys())]\n",
    "cm = CoherenceModel(topics=topics_for_gensim, texts=texts_tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()\n",
    "return coherence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def topic_diversity_score(model: BERTopic, top_n: int = 25):\n",
    "\"\"\"Compute topic diversity as (#unique words in top_n across topics) / (top_n * #topics).\n",
    "\n",
    "\n",
    "Higher means more diverse topics.\n",
    "\"\"\"\n",
    "topic_words = topic_top_words(model, top_n=top_n)\n",
    "all_words = [w for words in topic_words.values() for w in words]\n",
    "unique = set(all_words)\n",
    "n_topics = len(topic_words)\n",
    "denom = top_n * n_topics if n_topics > 0 else 1\n",
    "return len(unique) / denom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def distribution_stability(model: BERTopic, df: pd.DataFrame, embeddings: np.ndarray, n_splits: int = 5, sample_frac: float = 0.5):\n",
    "\"\"\"Estimate distribution stability by sampling and computing average JS divergence between topic distributions.\n",
    "\n",
    "\n",
    "We sample pairs and compute the Jensen-Shannon distance between their topic frequency distributions.\n",
    "\"\"\"\n",
    "rng = np.random.default_rng(RANDOM_STATE)\n",
    "js_scores = []\n",
    "for _ in range(n_splits):\n",
    "idx_a = rng.choice(len(df), size=int(len(df) * sample_frac), replace=False)\n",
    "idx_b = rng.choice(len(df), size=int(len(df) * sample_frac), replace=False)\n",
    "texts_a = df.iloc[idx_a]['clean_text'].tolist()\n",
    "texts_b = df.iloc[idx_b]['clean_text'].tolist()\n",
    "emb_a = embeddings[idx_a]\n",
    "emb_b = embeddings[idx_b]\n",
    "topics_a, _ = model.transform(texts_a, emb_a)\n",
    "topics_b, _ = model.transform(texts_b, emb_b)\n",
    "freq_a = Counter(topics_a)\n",
    "freq_b ="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
