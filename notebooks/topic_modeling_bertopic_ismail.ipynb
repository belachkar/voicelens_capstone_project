{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b52e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet bertopic sentence-transformers umap-learn hdbscan gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1dd7c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20  \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20  \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20  \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20  \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ismail/code/belachkar/voicelens_capstone_project/raw_data/review_for_amazon.csv\",\n",
    "                 encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Utils\n",
    "from typing import List\n",
    "\n",
    "# Load spaCy model (English)\n",
    "# For production, prefer: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cae58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean raw text: lowercase, remove URLs, punctuation, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize text using spaCy, removing stopwords & short tokens.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.lemma_ not in STOPWORDS\n",
    "        and len(token.lemma_) > 2\n",
    "        and token.lemma_.isalpha()\n",
    "    ]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4483a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(df: pd.DataFrame, text_column: str = \"comment\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply full preprocessing pipeline on the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): input dataframe\n",
    "        text_column (str): column containing raw text\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with processed_text column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Clean\n",
    "    df[\"clean_text\"] = df[text_column].apply(clean_text)\n",
    "\n",
    "    # Lemmatize\n",
    "    df[\"processed_text\"] = df[\"clean_text\"].apply(lemmatize_text)\n",
    "\n",
    "    # Drop rows where processed text is empty\n",
    "    df = df[df[\"processed_text\"].str.strip() != \"\"]\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "078167a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>uncaring and incompetent impossible to deal wi...</td>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon maybe the quickest way to get amazon ma...</td>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>not fair in genera i am an amazon junkie i lov...</td>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon prime is crap amazon prime is crap firs...</td>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>terrible delivery services terrible delivery s...</td>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  uncaring and incompetent impossible to deal wi...   \n",
       "1  amazon maybe the quickest way to get amazon ma...   \n",
       "2  not fair in genera i am an amazon junkie i lov...   \n",
       "3  amazon prime is crap amazon prime is crap firs...   \n",
       "4  terrible delivery services terrible delivery s...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  uncaring incompetent impossible deal customer ...  \n",
       "1  amazon maybe quick way get amazon maybe quick ...  \n",
       "2  fair genera amazon junkie love tthose package ...  \n",
       "3  amazon prime crap amazon prime crap first orde...  \n",
       "4  terrible delivery service terrible delivery se...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = preprocess_dataframe(df, text_column=\"comment\")\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be472105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2 ...\n",
      "Embedding model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---- Embedding Model for BERTopic ----\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_embedding_model(model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Load a sentence embedding model for BERTopic.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the SentenceTransformer model.\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name} ...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"Embedding model loaded successfully.\")\n",
    "    return model\n",
    "\n",
    "embedding_model = load_embedding_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349af6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTopic model successfully initialized.\n"
     ]
    }
   ],
   "source": [
    "# ---- Topic Modeling Step ----\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "def build_topic_model(\n",
    "    embedding_model,\n",
    "    n_neighbors: int = 15,\n",
    "    n_components: int = 5,\n",
    "    min_cluster_size: int = 10\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a BERTopic model with UMAP + HDBSCAN + SentenceTransformer embeddings.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: Preloaded SentenceTransformer model.\n",
    "        n_neighbors (int): UMAP neighbors.\n",
    "        n_components (int): UMAP dimensions.\n",
    "        min_cluster_size (int): Minimum cluster size for HDBSCAN.\n",
    "\n",
    "    Returns:\n",
    "        BERTopic: Configured BERTopic model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dimensionality Reduction\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Clustering\n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "\n",
    "    # BERTopic Model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"BERTopic model successfully initialized.\")\n",
    "    return topic_model\n",
    "\n",
    "\n",
    "# ---- Build the model ----\n",
    "topic_model = build_topic_model(embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33460e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 17:48:41,743 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BERTopic on 12948 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 405/405 [02:01<00:00,  3.33it/s]\n",
      "2025-11-18 17:50:43,758 - BERTopic - Embedding - Completed ✓\n",
      "2025-11-18 17:50:43,760 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-11-18 17:51:39,620 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-11-18 17:51:39,623 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-11-18 17:51:41,168 - BERTopic - Cluster - Completed ✓\n",
      "2025-11-18 17:51:41,189 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-11-18 17:51:42,144 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---- Fit BERTopic on your processed text ----\n",
    "\n",
    "def fit_topic_model(topic_model, df, text_column=\"processed_text\"):\n",
    "    \"\"\"\n",
    "    Fit BERTopic on cleaned text data.\n",
    "\n",
    "    Args:\n",
    "        topic_model: BERTopic model instance.\n",
    "        df (DataFrame): Preprocessed dataframe.\n",
    "        text_column (str): Column containing processed text.\n",
    "\n",
    "    Returns:\n",
    "        topics, probabilities, topic_info\n",
    "    \"\"\"\n",
    "    docs = df[text_column].tolist()\n",
    "    print(f\"Fitting BERTopic on {len(docs)} documents...\")\n",
    "\n",
    "    topics, probabilities = topic_model.fit_transform(docs)\n",
    "\n",
    "    print(\"Model fitted successfully.\")\n",
    "    topic_info = topic_model.get_topic_info()\n",
    "\n",
    "    return topics, probabilities, topic_info\n",
    "\n",
    "\n",
    "# ---- Run the fitting ----\n",
    "topics, probs, topic_info = fit_topic_model(topic_model, df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af975b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>topic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>uncaring and incompetent impossible to deal wi...</td>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon maybe the quickest way to get amazon ma...</td>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.927613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>not fair in genera i am an amazon junkie i lov...</td>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.885888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon prime is crap amazon prime is crap firs...</td>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>terrible delivery services terrible delivery s...</td>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  uncaring and incompetent impossible to deal wi...   \n",
       "1  amazon maybe the quickest way to get amazon ma...   \n",
       "2  not fair in genera i am an amazon junkie i lov...   \n",
       "3  amazon prime is crap amazon prime is crap firs...   \n",
       "4  terrible delivery services terrible delivery s...   \n",
       "\n",
       "                                      processed_text  dominant_topic  \\\n",
       "0  uncaring incompetent impossible deal customer ...               0   \n",
       "1  amazon maybe quick way get amazon maybe quick ...               0   \n",
       "2  fair genera amazon junkie love tthose package ...               0   \n",
       "3  amazon prime crap amazon prime crap first orde...               0   \n",
       "4  terrible delivery service terrible delivery se...               0   \n",
       "\n",
       "   topic_probability  \n",
       "0           0.825409  \n",
       "1           0.927613  \n",
       "2           0.885888  \n",
       "3           0.980481  \n",
       "4           0.968909  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assign_topics_to_df(df, topics, probabilities):\n",
    "    \"\"\"\n",
    "    Assigns BERTopic output back to the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Preprocessed dataframe\n",
    "        topics (list[int]): Topic numbers for each document\n",
    "        probabilities (np.ndarray or list): Probabilities from BERTopic\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with added columns:\n",
    "            - dominant_topic\n",
    "            - topic_probability (probability of dominant topic)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"dominant_topic\"] = topics\n",
    "\n",
    "    # If probabilities is 2D, take probability of the assigned topic\n",
    "    if isinstance(probabilities, np.ndarray) and probabilities.ndim == 2:\n",
    "        probs_for_dominant = [probabilities[i, t] if t >= 0 else 0.0\n",
    "                              for i, t in enumerate(topics)]\n",
    "        df[\"topic_probability\"] = probs_for_dominant\n",
    "    else:\n",
    "        df[\"topic_probability\"] = probabilities\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_topics = assign_topics_to_df(df_clean, topics, probs)\n",
    "df_topics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_labels(topic_model, topic_info, top_n_words: int = 3):\n",
    "    \"\"\"\n",
    "    Generate human-readable topic labels using BERTopic's top words.\n",
    "\n",
    "    Args:\n",
    "        topic_model: Fitted BERTopic model.\n",
    "        topic_info (DataFrame): topic_model.get_topic_info()\n",
    "        top_n_words (int): Number of representative words to include in the label.\n",
    "\n",
    "    Returns:\n",
    "        dict: {topic_id: topic_label}\n",
    "    \"\"\"\n",
    "    topic_labels = {}\n",
    "\n",
    "    for topic_id in topic_info.Topic:\n",
    "        if topic_id == -1:\n",
    "            topic_labels[topic_id] = \"Outliers\"\n",
    "            continue\n",
    "\n",
    "        # Get top words for this topic\n",
    "        words = [word for word, _ in topic_model.get_topic(topic_id)[:top_n_words]]\n",
    "\n",
    "        # Make a short, readable label\n",
    "        label = \" \".join(words).title()\n",
    "        topic_labels[topic_id] = label\n",
    "\n",
    "    return topic_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a164cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>Topic</th>\n",
       "      <th>topic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.825409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon maybe quick way get amazon maybe quick ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.927613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fair genera amazon junkie love tthose package ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.885888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.980481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.968909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day bait switch delivery nothing frustrating o...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.897454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ever use amazone ever use amazone faulty item ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.750962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cost return two item cost return two item defe...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.948280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>order small kitchen appliance order small kitc...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.897604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amazon need quit call number amazon need quit ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Amazon Service Customer</td>\n",
       "      <td>0.843111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_text  dominant_topic  \\\n",
       "0  uncaring incompetent impossible deal customer ...               0   \n",
       "1  amazon maybe quick way get amazon maybe quick ...               0   \n",
       "2  fair genera amazon junkie love tthose package ...               0   \n",
       "3  amazon prime crap amazon prime crap first orde...               0   \n",
       "4  terrible delivery service terrible delivery se...               0   \n",
       "5  day bait switch delivery nothing frustrating o...               0   \n",
       "6  ever use amazone ever use amazone faulty item ...               0   \n",
       "7  cost return two item cost return two item defe...               0   \n",
       "8  order small kitchen appliance order small kitc...               0   \n",
       "9  amazon need quit call number amazon need quit ...               0   \n",
       "\n",
       "                     Topic  topic_probability  \n",
       "0  Amazon Service Customer           0.825409  \n",
       "1  Amazon Service Customer           0.927613  \n",
       "2  Amazon Service Customer           0.885888  \n",
       "3  Amazon Service Customer           0.980481  \n",
       "4  Amazon Service Customer           0.968909  \n",
       "5  Amazon Service Customer           0.897454  \n",
       "6  Amazon Service Customer           0.750962  \n",
       "7  Amazon Service Customer           0.948280  \n",
       "8  Amazon Service Customer           0.897604  \n",
       "9  Amazon Service Customer           0.843111  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate labels\n",
    "topic_labels = generate_topic_labels(topic_model, topic_info)\n",
    "\n",
    "# Add a new column 'Topic' with human-readable labels\n",
    "df_topics[\"Topic\"] = df_topics[\"dominant_topic\"].map(topic_labels)\n",
    "\n",
    "df_topics[[\"processed_text\", \"dominant_topic\", \"Topic\", \"topic_probability\"]].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
