{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f250ef8",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling — Amazon Trustpilot Reviews\n",
    "\n",
    "**Purpose:** Perform efficient topic clustering using Latent Dirichlet Allocation (LDA) on Trustpilot Amazon reviews.  \n",
    "This notebook is a draft for team use: every cell has clear explanations so teammates can follow and reproduce results.\n",
    "\n",
    "**High-level steps:**\n",
    "1. Download dataset from Kaggle using kaggle CLI (curl-like via kaggle command)\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Preprocessing & cleaning\n",
    "4. Vectorization (CountVectorizer)\n",
    "5. Train LDA\n",
    "6. Inspect topics and assign dominant topic per review\n",
    "7. Visualize with pyLDAvis\n",
    "8. Save outputs for integration with backend/dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a65d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (27 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from nltk) (8.3.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Using cached wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pyLDAvis) (2.2.3)\n",
      "Collecting numexpr (from pyLDAvis)\n",
      "  Downloading numexpr-2.14.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting funcy (from pyLDAvis)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting gensim (from pyLDAvis)\n",
      "  Downloading gensim-4.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ismail/.pyenv/versions/3.10.6/envs/voicelens/lib/python3.10/site-packages (from jinja2->spacy) (3.0.3)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy-3.8.11-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (31.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (229 kB)\n",
      "Downloading murmurhash-1.0.15-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (122 kB)\n",
      "Downloading preshed-3.0.12-cp310-cp310-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (780 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.3/780.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.10-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gensim-4.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.6/27.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.14.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (440 kB)\n",
      "Using cached wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "Installing collected packages: funcy, wrapt, wasabi, typer-slim, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, scipy, regex, numexpr, murmurhash, joblib, cymem, cloudpathlib, catalogue, blis, srsly, smart-open, scikit-learn, preshed, nltk, gensim, confection, weasel, thinc, pyLDAvis, spacy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/28\u001b[0m [spacy]m27/28\u001b[0m [spacy]is]arn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 funcy-2.0 gensim-4.4.0 joblib-1.5.2 murmurhash-1.0.15 nltk-3.9.2 numexpr-2.14.1 preshed-3.0.12 pyLDAvis-3.4.1 regex-2025.11.3 scikit-learn-1.7.2 scipy-1.15.3 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 threadpoolctl-3.6.0 tqdm-4.67.1 typer-slim-0.20.0 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy scikit-learn pyLDAvis joblib\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Essential Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources (Run this cell first if you haven't yet!)\n",
    "try:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    print(\"NLTK resources not found. Downloading 'stopwords' and 'wordnet'...\")\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define global constants and file path\n",
    "FILE_PATH = \"/home/ismail/code/belachkar/voicelens_capstone_project/raw_data/review_for_amazon.csv\"\n",
    "ENCODING = 'latin-1'\n",
    "TARGET_COLUMN = 'topic' # Final column for descriptive topic string\n",
    "TEMP_ID_COLUMN = 'topic_id' # Temporary column for the numerical ID\n",
    "SCORE_COLUMN = 'topic_score'\n",
    "TEXT_COLUMN = 'comment' # Corrected text column\n",
    "RANDOM_STATE = 42\n",
    "OPTIMAL_K = 5 # Used for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb33e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure NLTK tokenizers are available (optional)\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9b69771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (12948, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12948 entries, 0 to 12947\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   name            12948 non-null  object\n",
      " 1   n_review        12948 non-null  int64 \n",
      " 2   country         12947 non-null  object\n",
      " 3   comment         12948 non-null  object\n",
      " 4   rating          12948 non-null  int64 \n",
      " 5   date            12948 non-null  object\n",
      " 6   processed_text  12948 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 708.2+ KB\n"
     ]
    }
   ],
   "source": [
    "## Method 1: Data Loading\n",
    "def load_data(file_path: str, encoding: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads the CSV file into a Pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=encoding)\n",
    "        if TEXT_COLUMN not in df.columns:\n",
    "            raise ValueError(f\"Column '{TEXT_COLUMN}' not found.\")\n",
    "        df.dropna(subset=[TEXT_COLUMN], inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "## Method 2: Text Preprocessing\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Cleans text: lowercasing, punctuation removal, stop word removal, and lemmatization.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply and Test the methods\n",
    "df = load_data(FILE_PATH, ENCODING)\n",
    "df['processed_text'] = df[TEXT_COLUMN].apply(preprocess_text)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LDA model with 5 topics...\n",
      "Training complete.\n",
      "Topic 0 Keywords: amazon, account, card, money, customer, gift, company, credit, email, prime\n",
      "Topic 1 Keywords: amazon, delivery, day, prime, order, item, time, package, shipping, get\n",
      "Topic 2 Keywords: amazon, service, great, always, good, customer, love, price, delivery, best\n",
      "Topic 3 Keywords: amazon, review, product, seller, item, return, bad, buy, good, get\n",
      "Topic 4 Keywords: customer, amazon, service, item, order, refund, time, would, get, told\n"
     ]
    }
   ],
   "source": [
    "## Method 3: Feature Vectorization (Count Vectorizer)\n",
    "def vectorize_text_for_lda(series: pd.Series, max_features: int = 5000):\n",
    "    \"\"\"Creates a Count Matrix (Document-Term Matrix) from the processed text for LDA.\"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(series)\n",
    "    return X, vectorizer\n",
    "\n",
    "## Method 4: Train LDA Topic Model\n",
    "def train_lda_model(X, n_topics: int, random_state: int) -> LatentDirichletAllocation:\n",
    "    \"\"\"Trains the Latent Dirichlet Allocation (LDA) model.\"\"\"\n",
    "    print(f\"\\nTraining LDA model with {n_topics} topics...\")\n",
    "    model = LatentDirichletAllocation(\n",
    "        n_components=n_topics,\n",
    "        max_iter=10,\n",
    "        learning_method='batch',\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X)\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "\n",
    "# Run Feature Engineering and Training\n",
    "X_counts, vectorizer = vectorize_text_for_lda(df['processed_text'])\n",
    "lda_model = train_lda_model(X_counts, OPTIMAL_K, RANDOM_STATE)\n",
    "\n",
    "## Method 5: Interpret Topics (Extract Keywords)\n",
    "def interpret_topics(model: LatentDirichletAllocation, vectorizer: CountVectorizer, top_n: int = 10):\n",
    "    \"\"\"Extracts the top-weighted words for each topic center.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics = {}\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_indices = topic.argsort()[:-top_n - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_indices]\n",
    "        topics[topic_idx] = top_words\n",
    "        print(f\"Topic {topic_idx} Keywords: {', '.join(top_words)}\")\n",
    "\n",
    "    return topics\n",
    "\n",
    "# Test the method\n",
    "topic_keywords_dict = interpret_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b665c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Manually Review & Name Topics ---\n",
      "Topic 0 (Keywords: amazon, account, card, money, customer) -> Assigned Name: 'Amazon Account Card'\n",
      "Topic 1 (Keywords: amazon, delivery, day, prime, order) -> Assigned Name: 'Amazon Delivery Day'\n",
      "Topic 2 (Keywords: amazon, service, great, always, good) -> Assigned Name: 'Amazon Service Great'\n",
      "Topic 3 (Keywords: amazon, review, product, seller, item) -> Assigned Name: 'Amazon Review Product'\n",
      "Topic 4 (Keywords: customer, amazon, service, item, order) -> Assigned Name: 'Customer Amazon Service'\n",
      "\n",
      "Sample Data with Final Descriptive Topics:\n",
      "                                             comment                    topic  \\\n",
      "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...  Customer Amazon Service   \n",
      "1  Amazon maybe the quickest way to get<U+0085>\\r...      Amazon Account Card   \n",
      "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...      Amazon Account Card   \n",
      "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...      Amazon Delivery Day   \n",
      "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...      Amazon Delivery Day   \n",
      "\n",
      "   topic_score  \n",
      "0     0.552300  \n",
      "1     0.535572  \n",
      "2     0.403145  \n",
      "3     0.531605  \n",
      "4     0.707978  \n",
      "\n",
      "Final Topic Distribution:\n",
      "topic\n",
      "Amazon Service Great       4180\n",
      "Customer Amazon Service    3174\n",
      "Amazon Delivery Day        2026\n",
      "Amazon Account Card        1941\n",
      "Amazon Review Product      1627\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## NEW Method 6: Create Descriptive Topic Mapping\n",
    "def create_topic_mapping(topic_keywords_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping dictionary from numerical ID to a descriptive string label.\n",
    "\n",
    "    NOTE: In a real-world scenario, a human analyst would review the keywords\n",
    "    and manually assign these names for better accuracy.\n",
    "    \"\"\"\n",
    "    topic_mapping = {}\n",
    "    print(\"\\n--- Manually Review & Name Topics ---\")\n",
    "\n",
    "    for topic_id, keywords in topic_keywords_dict.items():\n",
    "        # Heuristic Naming: Use the top 3 keywords concatenated\n",
    "        # REPLACE THIS LOGIC with manual names after reviewing the keywords\n",
    "        name = \" \".join(keywords[:3]).title()\n",
    "        topic_mapping[topic_id] = name\n",
    "        print(f\"Topic {topic_id} (Keywords: {', '.join(keywords[:5])}) -> Assigned Name: '{name}'\")\n",
    "\n",
    "    return topic_mapping\n",
    "\n",
    "# Create the mapping\n",
    "topic_name_map = create_topic_mapping(topic_keywords_dict)\n",
    "\n",
    "\n",
    "## Method 7 (Updated): Assign Topics and Descriptive Labels\n",
    "def assign_topics(df: pd.DataFrame, model: LatentDirichletAllocation, X_counts, topic_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Assigns the numerical topic ID, confidence score, and the final descriptive topic string.\n",
    "    \"\"\"\n",
    "    X_topic_distribution = model.transform(X_counts)\n",
    "\n",
    "    # 1. Assign Numerical ID and Score\n",
    "    df[TEMP_ID_COLUMN] = X_topic_distribution.argmax(axis=1)\n",
    "    df[SCORE_COLUMN] = X_topic_distribution.max(axis=1)\n",
    "\n",
    "    # 2. Assign Descriptive String Label (The Final Target Column)\n",
    "    df[TARGET_COLUMN] = df[TEMP_ID_COLUMN].map(topic_map)\n",
    "\n",
    "    # Drop the temporary numerical ID column\n",
    "    df.drop(columns=[TEMP_ID_COLUMN], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test the final assignment method\n",
    "df = assign_topics(df, lda_model, X_counts, topic_name_map)\n",
    "\n",
    "print(\"\\nSample Data with Final Descriptive Topics:\")\n",
    "print(df[[TEXT_COLUMN, TARGET_COLUMN, SCORE_COLUMN]].head())\n",
    "\n",
    "# Check final topic distribution\n",
    "print(\"\\nFinal Topic Distribution:\")\n",
    "print(df[TARGET_COLUMN].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LDA Model Evaluation Metrics ---\n",
      "Perplexity (Lower is Better): 923.50\n",
      "Log Likelihood (Higher is Better): -3517017.54\n"
     ]
    }
   ],
   "source": [
    "## Method 8: Evaluate LDA Model\n",
    "def evaluate_lda(model: LatentDirichletAllocation, X):\n",
    "    \"\"\"Calculates internal LDA metrics (Perplexity and Log-Likelihood).\"\"\"\n",
    "\n",
    "    perplexity = model.perplexity(X)\n",
    "    log_likelihood = model.score(X)\n",
    "\n",
    "    print(\"\\n--- LDA Model Evaluation Metrics ---\")\n",
    "    print(f\"Perplexity (Lower is Better): {perplexity:.2f}\")\n",
    "    print(f\"Log Likelihood (Higher is Better): {log_likelihood:.2f}\")\n",
    "\n",
    "# Test the method\n",
    "evaluate_lda(lda_model, X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80a60967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic_score</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>uncaring incompetent impossible deal customer ...</td>\n",
       "      <td>0.552300</td>\n",
       "      <td>Customer Amazon Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon maybe quickest way getu amazon maybe qu...</td>\n",
       "      <td>0.535572</td>\n",
       "      <td>Amazon Account Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>fair genus amazon junkie love tthose package c...</td>\n",
       "      <td>0.403145</td>\n",
       "      <td>Amazon Account Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>amazon prime crap amazon prime crap first orde...</td>\n",
       "      <td>0.531605</td>\n",
       "      <td>Amazon Delivery Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>terrible delivery service terrible delivery se...</td>\n",
       "      <td>0.707978</td>\n",
       "      <td>Amazon Delivery Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "                                      processed_text  topic_score  \\\n",
       "0  uncaring incompetent impossible deal customer ...     0.552300   \n",
       "1  amazon maybe quickest way getu amazon maybe qu...     0.535572   \n",
       "2  fair genus amazon junkie love tthose package c...     0.403145   \n",
       "3  amazon prime crap amazon prime crap first orde...     0.531605   \n",
       "4  terrible delivery service terrible delivery se...     0.707978   \n",
       "\n",
       "                     topic  \n",
       "0  Customer Amazon Service  \n",
       "1      Amazon Account Card  \n",
       "2      Amazon Account Card  \n",
       "3      Amazon Delivery Day  \n",
       "4      Amazon Delivery Day  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  \\\n",
      "0      Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...   \n",
      "1      Amazon maybe the quickest way to get<U+0085>\\r...   \n",
      "2      Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...   \n",
      "3      Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...   \n",
      "4      Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...   \n",
      "...                                                  ...   \n",
      "12943  Fast!!\\r\\n\\r\\nI have had perfect order fulfill...   \n",
      "12944  Consistently Excellent\\r\\n\\r\\nI have had perfe...   \n",
      "12945  Good prices but delivery can take time :(\\r\\n\\...   \n",
      "12946  World-class online shopping\\r\\n\\r\\nI have plac...   \n",
      "12947  No title\\r\\n\\r\\nthose goods i've ordered by Am...   \n",
      "\n",
      "                         topic  \n",
      "0      Customer Amazon Service  \n",
      "1          Amazon Account Card  \n",
      "2          Amazon Account Card  \n",
      "3          Amazon Delivery Day  \n",
      "4          Amazon Delivery Day  \n",
      "...                        ...  \n",
      "12943     Amazon Service Great  \n",
      "12944     Amazon Service Great  \n",
      "12945      Amazon Delivery Day  \n",
      "12946     Amazon Service Great  \n",
      "12947     Amazon Service Great  \n",
      "\n",
      "[12948 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[['comment','topic']])\n",
    "for row in df[['comment','topic']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dd6260e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>topic</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible to deal with customer service. I purchased an echo show as a gift for an elderly, blind lady.\\r\\r\\nAfter a power cut the machine would not reconnect.\\r\\r\\n\\r\\r\\nI tried to help but Amazon blocked her account.\\r\\r\\n\\r\\r\\nNumerous attempts to reset,many attempts to deal with Amazon service -all fobbed off to various other departments.\\r\\r\\n\\r\\r\\nA blind elderly lady has been ignored and no help given.\\r\\r\\n\\r\\r\\nShame on Amazon</td>\n",
       "      <td>Customer Amazon Service</td>\n",
       "      <td>uncaring incompetent impossible deal customer service purchased echo show gift elderly blind lady power cut machine would reconnect tried help amazon blocked account numerous attempt resetmany attempt deal amazon service fobbed various department blind elderly lady ignored help given shame amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r\\r\\n\\r\\r\\nAmazon maybe the quickest way to get what you want but it isn&lt;U+0092&gt;t always the best option; customer service is frankly overall poor; frequent contact after purchase is necessary to report poor construction, inferior materials and specification in the face of Amazon&lt;U+0092&gt;s massive under-cutting of UK manufacturers; undermining the value of the UK highstreet just to get back your hard-earner cash. Agents are sometime helpful but often slow to act when challenged over repeated account violations by Sellers who try to interfere with the returns process or who object to some reviews; Amazon and it&lt;U+0092&gt;s Sellers are overly sensitive to criticism; I&lt;U+0092&gt;ve finally pulled the plug on online purchases from the big A, cancelled my Prime package and frozen my Amazon New Day linked credit card account after repeated attempts to take money not authorised by me. Had to invoke fraud investigation to stop this worrying account activity and force an investigation. Outcome unsatisfactory!!</td>\n",
       "      <td>Amazon Account Card</td>\n",
       "      <td>amazon maybe quickest way getu amazon maybe quickest way get want isnut always best option customer service frankly overall poor frequent contact purchase necessary report poor construction inferior material specification face amazonus massive undercutting manufacturer undermining value highstreet get back hardearner cash agent sometime helpful often slow act challenged repeated account violation seller try interfere return process object review amazon itus seller overly sensitive criticism iuve finally pulled plug online purchase big cancelled prime package frozen amazon new day linked credit card account repeated attempt take money authorised invoke fraud investigation stop worrying account activity force investigation outcome unsatisfactory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon junkie. I love tthose packages coming in day after day. I think they have one of the best delivery systems in the business. So why a one star? When they protect sellers so customers can't post an honest review I think that is a dis-service. I recently received a gift which would have benefited from an honest review but Amazon blocked it by saying there was an unusual level of activity so only reviews by senders could be written. Duh, how could they know? Not fair. I was blocked from writing a review, being the recipient of the gift.</td>\n",
       "      <td>Amazon Account Card</td>\n",
       "      <td>fair genus amazon junkie love tthose package coming day day think one best delivery system business one star protect seller customer cant post honest review think disservice recently received gift would benefited honest review amazon blocked saying unusual level activity review sender could written duh could know fair blocked writing review recipient gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime is crap first of all I order an Al Mar knife for $140 and then what I get is a counterfeit $30 knife I send it back and they tell me I may have to wait 30 days to get my money back! Second thing I order 4 air filters for my RV A/C and I get the package and there's two filters in there they want me to take my filter out put my dirty one back in my air conditioner and send it back to them for them to do anything about it. I'm tired of their incompetence, and they're Crooks!</td>\n",
       "      <td>Amazon Delivery Day</td>\n",
       "      <td>amazon prime crap amazon prime crap first order mar knife get counterfeit knife send back tell may wait day get money back second thing order air filter get package there two filter want take filter put dirty one back air conditioner send back anything tired incompetence theyre crook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible delivery services and very inconsistent of where deliveries are left at a presice location.\\r\\r\\nI would estimate 95% of my deliveries are not at the\\r\\r\\nlocation i left in directions i left. Most diver's do not read the simple instructions, their to busy.</td>\n",
       "      <td>Amazon Delivery Day</td>\n",
       "      <td>terrible delivery service terrible delivery service inconsistent delivery left presice location would estimate delivery location left direction left diver read simple instruction busy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          comment  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible to deal with customer service. I purchased an echo show as a gift for an elderly, blind lady.\\r\\r\\nAfter a power cut the machine would not reconnect.\\r\\r\\n\\r\\r\\nI tried to help but Amazon blocked her account.\\r\\r\\n\\r\\r\\nNumerous attempts to reset,many attempts to deal with Amazon service -all fobbed off to various other departments.\\r\\r\\n\\r\\r\\nA blind elderly lady has been ignored and no help given.\\r\\r\\n\\r\\r\\nShame on Amazon   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r\\r\\n\\r\\r\\nAmazon maybe the quickest way to get what you want but it isn<U+0092>t always the best option; customer service is frankly overall poor; frequent contact after purchase is necessary to report poor construction, inferior materials and specification in the face of Amazon<U+0092>s massive under-cutting of UK manufacturers; undermining the value of the UK highstreet just to get back your hard-earner cash. Agents are sometime helpful but often slow to act when challenged over repeated account violations by Sellers who try to interfere with the returns process or who object to some reviews; Amazon and it<U+0092>s Sellers are overly sensitive to criticism; I<U+0092>ve finally pulled the plug on online purchases from the big A, cancelled my Prime package and frozen my Amazon New Day linked credit card account after repeated attempts to take money not authorised by me. Had to invoke fraud investigation to stop this worrying account activity and force an investigation. Outcome unsatisfactory!!   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon junkie. I love tthose packages coming in day after day. I think they have one of the best delivery systems in the business. So why a one star? When they protect sellers so customers can't post an honest review I think that is a dis-service. I recently received a gift which would have benefited from an honest review but Amazon blocked it by saying there was an unusual level of activity so only reviews by senders could be written. Duh, how could they know? Not fair. I was blocked from writing a review, being the recipient of the gift.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime is crap first of all I order an Al Mar knife for $140 and then what I get is a counterfeit $30 knife I send it back and they tell me I may have to wait 30 days to get my money back! Second thing I order 4 air filters for my RV A/C and I get the package and there's two filters in there they want me to take my filter out put my dirty one back in my air conditioner and send it back to them for them to do anything about it. I'm tired of their incompetence, and they're Crooks!   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible delivery services and very inconsistent of where deliveries are left at a presice location.\\r\\r\\nI would estimate 95% of my deliveries are not at the\\r\\r\\nlocation i left in directions i left. Most diver's do not read the simple instructions, their to busy.   \n",
       "\n",
       "                     topic  \\\n",
       "0  Customer Amazon Service   \n",
       "1      Amazon Account Card   \n",
       "2      Amazon Account Card   \n",
       "3      Amazon Delivery Day   \n",
       "4      Amazon Delivery Day   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      processed_text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                          uncaring incompetent impossible deal customer service purchased echo show gift elderly blind lady power cut machine would reconnect tried help amazon blocked account numerous attempt resetmany attempt deal amazon service fobbed various department blind elderly lady ignored help given shame amazon  \n",
       "1  amazon maybe quickest way getu amazon maybe quickest way get want isnut always best option customer service frankly overall poor frequent contact purchase necessary report poor construction inferior material specification face amazonus massive undercutting manufacturer undermining value highstreet get back hardearner cash agent sometime helpful often slow act challenged repeated account violation seller try interfere return process object review amazon itus seller overly sensitive criticism iuve finally pulled plug online purchase big cancelled prime package frozen amazon new day linked credit card account repeated attempt take money authorised invoke fraud investigation stop worrying account activity force investigation outcome unsatisfactory  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                              fair genus amazon junkie love tthose package coming day day think one best delivery system business one star protect seller customer cant post honest review think disservice recently received gift would benefited honest review amazon blocked saying unusual level activity review sender could written duh could know fair blocked writing review recipient gift  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       amazon prime crap amazon prime crap first order mar knife get counterfeit knife send back tell may wait day get money back second thing order air filter get package there two filter want take filter put dirty one back air conditioner send back anything tired incompetence theyre crook  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            terrible delivery service terrible delivery service inconsistent delivery left presice location would estimate delivery location left direction left diver read simple instruction busy  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['comment','topic','processed_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd218e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12948 entries, 0 to 12947\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   name            12948 non-null  object \n",
      " 1   n_review        12948 non-null  int64  \n",
      " 2   country         12947 non-null  object \n",
      " 3   comment         12948 non-null  object \n",
      " 4   rating          12948 non-null  int64  \n",
      " 5   date            12948 non-null  object \n",
      " 6   processed_text  12948 non-null  object \n",
      " 7   topic_score     12948 non-null  float64\n",
      " 8   topic           12948 non-null  object \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 910.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
