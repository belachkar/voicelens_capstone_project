{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fd0029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20  \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20  \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20  \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20  \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ismail/code/belachkar/voicelens_capstone_project/raw_data/review_for_amazon.csv\",\n",
    "                 encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a09555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ismail/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic text cleaning: lowercase, remove URLs, special characters,\n",
    "    extra spaces, and keep alphabetic words only.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)       # remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)                 # keep letters only\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # remove extra spaces\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize using gensim simple_preprocess (handles punctuation).\n",
    "    \"\"\"\n",
    "    return simple_preprocess(text, deacc=True)\n",
    "\n",
    "def remove_stopwords(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Remove English stopwords.\n",
    "    \"\"\"\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def lemmatize_tokens(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Lemmatize tokens to reduce words to their base form.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def preprocess_pipeline(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline:\n",
    "    1. Clean text\n",
    "    2. Tokenize\n",
    "    3. Remove stopwords\n",
    "    4. Lemmatize\n",
    "    Returns list of final processed tokens.\n",
    "    \"\"\"\n",
    "    cleaned = clean_text(text)\n",
    "    tokens = tokenize(cleaned)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933c1531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 3244\n",
      "Example BOW for first document: [(0, 1), (1, 2), (2, 2), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 2)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Apply preprocessing to the \"comment\" column\n",
    "processed_docs = df['comment'].apply(preprocess_pipeline)\n",
    "\n",
    "# Create dictionary mapping words → ids\n",
    "dictionary = Dictionary(processed_docs)\n",
    "\n",
    "# Filter extremes to improve LDA quality\n",
    "dictionary.filter_extremes(\n",
    "    no_below=10,    # keep tokens that appear in at least 10 documents\n",
    "    no_above=0.5    # remove tokens that appear in more than 50% of documents\n",
    ")\n",
    "\n",
    "# Create Bag-of-Words corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "print(\"Dictionary size:\", len(dictionary))\n",
    "print(\"Example BOW for first document:\", corpus[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736db5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model successfully trained!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# Define number of topics (can tune later)\n",
    "NUM_TOPICS = 10\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    random_state=42,\n",
    "    chunksize=2000,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    eval_every=None  # disables per-iteration logging for speed\n",
    ")\n",
    "\n",
    "print(\"LDA model successfully trained!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "505da473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant topic successfully assigned to all rows!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>n_review</th>\n",
       "      <th>country</th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>Topic</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Graham MOORE</td>\n",
       "      <td>21</td>\n",
       "      <td>GB</td>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>popadog</td>\n",
       "      <td>5</td>\n",
       "      <td>GB</td>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Torok</td>\n",
       "      <td>6</td>\n",
       "      <td>US</td>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jerry Jocoy</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>steve erickson</td>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-06-19</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  n_review country  \\\n",
       "0    Graham MOORE        21      GB   \n",
       "1         popadog         5      GB   \n",
       "2    Andrew Torok         6      US   \n",
       "3     Jerry Jocoy        15      US   \n",
       "4  steve erickson         3      US   \n",
       "\n",
       "                                             comment  rating        date  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...       1  2022-06-20   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...       2  2022-06-20   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...       1  2022-06-20   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...       1  2022-06-20   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...       1  2022-06-19   \n",
       "\n",
       "   Topic  dominant_topic  \n",
       "0      9               9  \n",
       "1      0               0  \n",
       "2      2               2  \n",
       "3      1               1  \n",
       "4      7               7  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dominant_topic(model, bow):\n",
    "    \"\"\"\n",
    "    Return the dominant topic ID for a single document.\n",
    "    \"\"\"\n",
    "    topic_probs = model.get_document_topics(bow)\n",
    "    if not topic_probs:\n",
    "        return None\n",
    "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "# Apply to entire dataset\n",
    "topic_ids = [get_dominant_topic(lda_model, bow) for bow in corpus]\n",
    "\n",
    "df['dominant_topic'] = topic_ids  # keep numeric IDs\n",
    "df['Topic'] = topic_ids           # temporary placeholder\n",
    "\n",
    "print(\"Dominant topic successfully assigned to all rows!\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc802177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'account, card, email, order',\n",
       " 1: 'prime, package, delivery, get',\n",
       " 2: 'review, product, book, star',\n",
       " 3: 'seller, company, product, money',\n",
       " 4: 'price, great, love, good',\n",
       " 5: 'would, said, told, back',\n",
       " 6: 'service, customer, good, always',\n",
       " 7: 'kindle, driver, parcel, delivery',\n",
       " 8: 'item, day, order, delivery',\n",
       " 9: 'customer, service, worst, company'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic_keywords(model, num_words=5):\n",
    "    \"\"\"\n",
    "    Return top keywords for each LDA topic as a readable label.\n",
    "    \"\"\"\n",
    "    topic_keywords = {}\n",
    "    for topic_id in range(model.num_topics):\n",
    "        words = model.show_topic(topic_id, topn=num_words)\n",
    "        keywords = \", \".join([word for word, _ in words])\n",
    "        topic_keywords[topic_id] = keywords\n",
    "    return topic_keywords\n",
    "\n",
    "\n",
    "# Get labels\n",
    "topic_labels = get_topic_keywords(lda_model, num_words=4)\n",
    "\n",
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ce60e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...</td>\n",
       "      <td>9</td>\n",
       "      <td>customer, service, worst, company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon maybe the quickest way to get&lt;U+0085&gt;\\r...</td>\n",
       "      <td>0</td>\n",
       "      <td>account, card, email, order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...</td>\n",
       "      <td>2</td>\n",
       "      <td>review, product, book, star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...</td>\n",
       "      <td>1</td>\n",
       "      <td>prime, package, delivery, get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...</td>\n",
       "      <td>7</td>\n",
       "      <td>kindle, driver, parcel, delivery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  dominant_topic  \\\n",
       "0  Uncaring and incompetent\\r\\r\\n\\r\\r\\nImpossible...               9   \n",
       "1  Amazon maybe the quickest way to get<U+0085>\\r...               0   \n",
       "2  Not fair!\\r\\r\\n\\r\\r\\nIn genera! I am an Amazon...               2   \n",
       "3  Amazon Prime is crap\\r\\r\\n\\r\\r\\nAmazon Prime i...               1   \n",
       "4  Terrible delivery services\\r\\r\\n\\r\\r\\nTerrible...               7   \n",
       "\n",
       "                               Topic  \n",
       "0  customer, service, worst, company  \n",
       "1        account, card, email, order  \n",
       "2        review, product, book, star  \n",
       "3      prime, package, delivery, get  \n",
       "4   kindle, driver, parcel, delivery  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================================================\n",
    "# SECTION 7 — MAP TOPIC IDs TO HUMAN LABELS\n",
    "# =======================================================\n",
    "\n",
    "def assign_topic_labels(df, topic_id_col, topic_labels_dict):\n",
    "    \"\"\"\n",
    "    Map numeric LDA topic IDs to human-readable topic labels.\n",
    "    \"\"\"\n",
    "    df[\"Topic\"] = df[topic_id_col].map(topic_labels_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply mapping\n",
    "df = assign_topic_labels(df, \"dominant_topic\", topic_labels)\n",
    "\n",
    "df[[\"comment\", \"dominant_topic\", \"Topic\"]].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voicelens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
